{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV = True\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, ParameterSampler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score as auc\n",
    "from sklearn.metrics import fbeta_score, cohen_kappa_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.observer import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "from bayes_opt.util import load_logs\n",
    "\n",
    "def lgb_f1_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n",
    "    return 'f1', f1_score(y_true, y_hat), True\n",
    "\n",
    "def ensure_dir(file_path):\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "input_path = \"../input/\"\n",
    "output_path = \"../output/\"\n",
    "model_path = \"../models/\"\n",
    "\n",
    "ensure_dir(model_path)\n",
    "ensure_dir(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(input_path+'train.csv.zip')\n",
    "\n",
    "label = train_df.target\n",
    "train = train_df.drop(['ID_code','target'],axis=1)\n",
    "#train = train_df.drop(['ID_code'],axis=1)\n",
    "\n",
    "test = pd.read_csv(input_path+'test.csv.zip')\n",
    "test = test.drop(['ID_code'],axis=1)\n",
    "\n",
    "test_filtered = pd.read_pickle(input_path+'test_filtered.pkl')\n",
    "test_filtered = test_filtered.loc[:,train.columns]\n",
    "\n",
    "train_test = pd.concat([train,test_filtered]).reset_index(drop=True)\n",
    "#train_test.target.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in tqdm(range(200)):\n",
    "#     col = \"var_\" + str(i) + \"_train_test_sum_vcs_prod\"\n",
    "#     train_test[[\"target\", col]].sort_values(by=col).to_csv(output_path+col+\".csv\", index=False)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 53.21it/s]\n"
     ]
    }
   ],
   "source": [
    "vcs_train_test = {}\n",
    "mean_target = {}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_index, valid_index in skf.split(train_df, train_df.target):\n",
    "    break\n",
    "    \n",
    "train_tmp = train_df.loc[train_index]\n",
    "\n",
    "for i in tqdm(range(200)):\n",
    "    col = \"var_\"+str(i)\n",
    "    vcs_train_test[col] = train_test.loc[:,col].value_counts()\n",
    "    \n",
    "#     train_tmp[col+\"_round\"] = train_tmp[col].apply(lambda x: round(x,0))\n",
    "    \n",
    "#     d0=dict(train_tmp.loc[train_tmp.target==0][col+\"_round\"].value_counts())\n",
    "#     d1=dict(train_tmp.loc[train_tmp.target==1][col+\"_round\"].value_counts())\n",
    "    \n",
    "#     mean_target[col] = dict()\n",
    "#     for k in sorted(list(set(d0)&set(d1))):\n",
    "#         mean_target[col][k] = d1[k]/d0[k]\n",
    "              \n",
    "#     vtraintest = vcs_train_test[col]\n",
    "#     t = vtraintest[train_test[col]].values\n",
    "#     train_test[col+'_train_test_sum_vcs'] = t\n",
    "#     train_test[col+'_train_test_sum_vcs_prod'] = train_test[col]*t\n",
    "#     vcs_train_test_prod[col] = train_test.loc[:,col+'_train_test_sum_vcs_prod'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:04<00:00, 40.65it/s]\n",
      "100%|██████████| 200/200 [00:04<00:00, 29.48it/s]\n"
     ]
    }
   ],
   "source": [
    "def feature_generator(df):\n",
    "    for i in tqdm(range(200)):\n",
    "        col = \"var_\"+str(i)\n",
    "        \n",
    "        vtraintest = vcs_train_test[col]\n",
    "        t = vtraintest[df[col]].values #fillna(0).values\n",
    "        df[col+'_train_test_sum_vcs'] = t\n",
    "        df[col+'_train_test_sum_vcs_prod'] = df[col] * t\n",
    "        \n",
    "#         t = df[col].apply(lambda x: round(x,0)).apply(lambda x: mean_target[col].get(x,0.11)).values\n",
    "#         df[col+'_train_test_mean_target'] = t\n",
    "\n",
    "#         df[col+'_diff'] = df[col]-np.median(df[col])\n",
    "#         vtraintest_prod = vcs_train_test_prod[col]\n",
    "#         t = vtraintest_prod[df[col]].values #fillna(0).values\n",
    "#         df[col+'_train_test_sum_vcs_'] = t\n",
    "#         df[col+'_train_test_sum_vcs_prod_'] = df[col]*t\n",
    "        \n",
    "#         df[col+'_train_test_sum_vcs_sign'] = (df[col+\"_train_test_sum_vcs_prod\"]>0).astype(int)\n",
    "#         df[col+'_train_test_sum_vcs_div'] = df[col]/t\n",
    "#         df[col+'_train_test_sum_vcs_minus'] = scale(df[col]) - scale(t)\n",
    "#         df[col+'_train_test_sum_vcs_plus'] = scale(df[col]) + scale(t)\n",
    "#         df[col+'_train_test_sum_vcs_min'] = np.min(scale(df[col]), scale(t))\n",
    "#         df[col+'_train_test_sum_vcs_max'] = np.max(scale(df[col]), scale(t))\n",
    "#         df[col+'_train_test_sum_vcs_pow'] = np.power(abs(scale(df[col])), abs(scale(t)))\n",
    "#         df[col+'_train_test_sum_vcs_log'] = np.log(abs(scale(df[col])), abs(scale(t)))\n",
    "        \n",
    "feature_generator(train)\n",
    "feature_generator(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    179902\n",
       "1     20098\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = train\n",
    "dt = test\n",
    "\n",
    "cols = sorted(list(set(df.columns)-{\"label\"}))\n",
    "print(len(cols))\n",
    "\n",
    "df[\"label\"] = label\n",
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.var_0_train_test_mean_target.hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 12\n",
    "\n",
    "train[f\"var_{n}_round\"] = train[f\"var_{n}\"].apply(lambda x: round(x,1))\n",
    "print(len(train[f\"var_{n}_round\"].value_counts()))\n",
    "\n",
    "col = f\"var_{n}_round\"\n",
    "\n",
    "d0=dict(train[train.label==0][col].value_counts())\n",
    "d1=dict(train[train.label==1][col].value_counts())\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for k in sorted(list(set(d0)&set(d1))):\n",
    "    res = d1[k]/d0[k]\n",
    "    #print(k, res, d0[k])\n",
    "    x.append(k)\n",
    "    y.append(res)\n",
    "    \n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler() #MinMaxScaler StandardScaler RobustScaler\n",
    "\n",
    "X = scaler.fit_transform(df[cols])\n",
    "#Z = scaler.transform(dt[cols])\n",
    "y = list(df.label)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.876 \n",
      "F-score: 0.4295 0\n",
      "CPU times: user 1.45 s, sys: 5.89 s, total: 7.34 s\n",
      "Wall time: 4min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df[\"clf\"] = 0\n",
    "dt[\"clf\"] = 0\n",
    "\n",
    "i = -1\n",
    "for train_index, valid_index in skf.split(X, y):\n",
    "    i+=1\n",
    "        \n",
    "    X_train = X[train_index, :]\n",
    "    X_valid = X[valid_index, :]\n",
    "\n",
    "    y_train = df.loc[train_index, \"label\"]\n",
    "    y_valid = df.loc[valid_index, \"label\"]\n",
    "    \n",
    "    fname = model_path+'clf_'+str(i)+'.pkl'\n",
    "    if DEV or not isfile(fname):\n",
    "        c = 10\n",
    "        clf = LogisticRegression(C=c,\n",
    "                                 solver=\"newton-cg\",\n",
    "                                 penalty=\"l2\", \n",
    "                                 n_jobs=-1, \n",
    "                                 max_iter=100).fit(X_train, y_train) \n",
    "        joblib.dump(clf, fname)\n",
    "    else:\n",
    "        clf = joblib.load(fname)\n",
    "\n",
    "    y_pred = clf.predict_proba(X_valid)[:,1] \n",
    "    df.loc[valid_index, \"clf\"] = y_pred\n",
    "    print(\"ROC AUC:\", round(auc(y_valid, y_pred), 4), \n",
    "          \"\\nF-score:\", round(fbeta_score(y_valid, y_pred>0.5, beta=1), 4), i)\n",
    "    \n",
    "    #dt[\"clf\"] += clf.predict(Z)/5\n",
    "    break\n",
    "    \n",
    "\n",
    "# print(\"\\nROC AUC:\", round(auc(df.label, df[\"clf\"]), 4), \n",
    "#       \"\\nF-score:\", round(fbeta_score(df.label, df[\"clf\"]>0.5, beta=1), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols+=[\"clf\"]\n",
    "\n",
    "scaler = StandardScaler() #MinMaxScaler StandardScaler RobustScaler\n",
    "\n",
    "X = scaler.fit_transform(df[cols])\n",
    "Z = scaler.transform(dt[cols])\n",
    "y = list(df.label)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df[\"svc\"] = 0\n",
    "dt[\"svc\"] = 0\n",
    "\n",
    "i = -1\n",
    "for train_index, valid_index in skf.split(X, y):\n",
    "    i+=1\n",
    "        \n",
    "    X_train = X[train_index, :]\n",
    "    X_valid = X[valid_index, :]\n",
    "\n",
    "    y_train = df.loc[train_index, \"label\"]\n",
    "    y_valid = df.loc[valid_index, \"label\"]\n",
    "    \n",
    "    fname = model_path+'svc_'+str(i)+'.pkl'\n",
    "    #print(fname)\n",
    "    if DEV or not isfile(fname):\n",
    "        c = 100\n",
    "        svc = SVC(C=c,\n",
    "                  probability=True).fit(X_train, y_train) \n",
    "        \n",
    "        joblib.dump(svc, fname)\n",
    "    else:\n",
    "        svc = joblib.load(fname)\n",
    "\n",
    "    y_pred = svc.predict_proba(X_valid)[:,1] \n",
    "    df.loc[valid_index, \"svc\"] = y_pred\n",
    "    print(\"ROC AUC:\", round(auc(y_valid, y_pred), 4), \n",
    "          \"\\nF-score:\", round(fbeta_score(y_valid, y_pred>0.5, beta=1), 4), i)\n",
    "    \n",
    "    #dt[\"svc\"] += svc.predict(Z)/5\n",
    "    break\n",
    "\n",
    "\n",
    "# print(\"\\nROC AUC:\", round(auc(df.label, df[\"svc\"]), 4), \n",
    "#       \"\\nF-score:\", round(fbeta_score(df.label, df[\"svc\"]>0.5, beta=1), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols+=[\"svc\"]\n",
    "\n",
    "scaler = StandardScaler() #MinMaxScaler StandardScaler RobustScaler\n",
    "\n",
    "X = scaler.fit_transform(df[cols])\n",
    "#Z = scaler.transform(dt[cols])\n",
    "y = list(df.label)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df[\"nei\"] = 0\n",
    "dt[\"nei\"] = 0\n",
    "\n",
    "i = -1\n",
    "for train_index, valid_index in skf.split(X, y):\n",
    "    i+=1\n",
    "        \n",
    "    X_train = X[train_index, :]\n",
    "    X_valid = X[valid_index, :]\n",
    "\n",
    "    y_train = df.loc[train_index, \"label\"]\n",
    "    y_valid = df.loc[valid_index, \"label\"]\n",
    "    \n",
    "    fname = model_path+'nei_'+str(i)+'.pkl'\n",
    "    if DEV or not isfile(fname):\n",
    "        nei = KNeighborsClassifier(n_neighbors=2,\n",
    "                                   p=1,n_jobs=-1).fit(X_train, y_train) \n",
    "        joblib.dump(nei, fname)\n",
    "    else:\n",
    "        nei = joblib.load(fname)\n",
    "\n",
    "    y_pred = nei.predict_proba(X_valid)[:,1] \n",
    "    df.loc[valid_index, \"nei\"] = y_pred\n",
    "    print(\"ROC AUC:\", round(auc(y_valid, y_pred), 4), \n",
    "          \"\\nF-score:\", round(fbeta_score(y_valid, y_pred>0.5, beta=1), 4), i)\n",
    "    \n",
    "    #dt[\"nei\"] += nei.predict(Z)/5\n",
    "    break\n",
    "\n",
    "# print(\"\\nROC AUC:\", round(auc(df.label, df[\"nei\"]), 4), \n",
    "#       \"\\nF-score:\", round(fbeta_score(df.label, df[\"nei\"]>0.5, beta=1), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LighGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols+=[\"nei\"]\n",
    "\n",
    "# k = 100\n",
    "# cols = [f\"var_{k}\",\n",
    "#         # f\"var_{k}_train_test_sum_vcs\",\n",
    "#         # f\"var_{k}_train_test_sum_vcs_prod\"\n",
    "#        ]\n",
    "\n",
    "scaler = StandardScaler() #MinMaxScaler StandardScaler RobustScaler\n",
    "\n",
    "X = df[cols].values #scaler.fit_transform(df[cols])\n",
    "#Z = dt[cols # ]scaler.transform(dt[cols])\n",
    "y = list(df.label)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds = 10000\n",
    "early_stop_rounds = 200\n",
    "\n",
    "params = {'lambda_l1': 0, \n",
    "          'lambda_l2': 0,\n",
    "          'feature_fraction':0.9,\n",
    "#           'bagging_fraction':0.1,\n",
    "#           'bagging_freq': 1,\n",
    "          'learning_rate': 0.03, \n",
    "          'max_depth': 4,\n",
    "          #'min_data_in_leaf':20,\n",
    "          #'num_leaves':2**5-1,\n",
    "          'boosting_type': 'gbrt', #dart gbrt\n",
    "          'objective': 'binary', \n",
    "          'metric': 'auc',\n",
    "          #'weight': [1, 0.5],\n",
    "          #'device': 'gpu',\n",
    "          #'gpu_platform_id': '0',\n",
    "          #'gpu_device_id': '0',\n",
    "          'max_bin': 1024,\n",
    "          'n_jobs':-1\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df[\"lgb1\"] = 0\n",
    "dt[\"lgb\"] = 0\n",
    "\n",
    "i = -1\n",
    "for train_index, valid_index in skf.split(X, y):\n",
    "    i+=1\n",
    "        \n",
    "    X_train = X[train_index, :]\n",
    "    X_valid = X[valid_index, :]\n",
    "\n",
    "    y_train = df.loc[train_index, \"label\"]\n",
    "    y_valid = df.loc[valid_index, \"label\"]\n",
    "\n",
    "    d_train = lgb.Dataset(X_train, y_train, feature_name=cols)\n",
    "    d_valid = lgb.Dataset(X_valid, y_valid, feature_name=cols)    \n",
    "    \n",
    "    fname = model_path+'lgb_'+str(i)+'.pkl'\n",
    "    if DEV or not isfile(fname):\n",
    "        model = lgb.train(params,\n",
    "                            d_train,\n",
    "                            num_boost_round=rounds,\n",
    "                            valid_sets=[d_train, d_valid],\n",
    "                            valid_names=['train','valid'],\n",
    "                            #feval=lgb_f1_score,\n",
    "                            early_stopping_rounds=early_stop_rounds,\n",
    "                            verbose_eval=100)\n",
    "        \n",
    "        joblib.dump(model, fname)\n",
    "    else:\n",
    "        model = joblib.load(fname)\n",
    "    \n",
    "    \n",
    "    y_pred = model.predict(X_valid)\n",
    "    df.loc[valid_index, \"lgb1\"] = y_pred\n",
    "    print(\"ROC AUC:\", round(auc(y_valid, y_pred), 4), \n",
    "      \"\\nF-score:\", round(fbeta_score(y_valid, y_pred>0.5, beta=1), 4), i)\n",
    "    \n",
    "    #dt[\"lgb\"] += model.predict(Z)/5\n",
    "    break\n",
    "    \n",
    "# print(\"\\nROC AUC:\", round(auc(df.label, df[\"lgb\"]), 4), \n",
    "#       \"\\nF-score:\", round(fbeta_score(df.label, df[\"lgb\"]>0.5, beta=1), 4))\n",
    "\n",
    "# [3992]\ttrain's auc: 0.989264\tvalid's auc: 0.913532\n",
    "# ROC AUC: 0.9135 \n",
    "# F-score: 0.5195 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[valid_index, \"lgb\"][df.label==1][df.lgb>0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [1,2]\n",
    "\n",
    "df[\"avg\"] = (w[0]*df[\"lgb\"]+w[1]*df[\"lgb1\"])/sum(w)\n",
    "\n",
    "print(\"\\nROC AUC:\", round(auc(y_valid, df.loc[valid_index, \"avg\"]), 4))\n",
    "print(\"\\nROC AUC:\", round(auc(y_valid, df.loc[valid_index, \"lgb\"]), 4))\n",
    "print(\"\\nROC AUC:\", round(auc(y_valid, df.loc[valid_index, \"lgb1\"]), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(dt[\"avg\"] > 0.5) / len(dt), np.sum(df[\"avg\"] > 0.5) / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = list(dt[\"avg\"] > 0.5)\n",
    "np.mean(predictions[:2*len(predictions)//5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt[[\"Image\",\"avg\"]].to_csv(\"test_prediction_top5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 300))\n",
    "lgb.plot_importance(model, max_num_features=len(cols), ax=ax)\n",
    "plt.title(\"Light GBM Feature Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df[\"lgb\"]>0.5)&(df.label!=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[(df[\"lgb\"]<0.5)&(df.label==1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr().label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler() #MinMaxScaler StandardScaler RobustScaler\n",
    "\n",
    "X = scaler.fit_transform(df[cols])\n",
    "Z = scaler.transform(dt[cols])\n",
    "y = list(df.label)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {  \n",
    "              'max_depth': [3,4,5],\n",
    "              'learning_rate': np.linspace(0.01, 0.1, 10),\n",
    "              'feature_fraction': np.linspace(0.5, 0.9, 5),\n",
    "              'lambda_l1': np.linspace(1, 10, 10),\n",
    "              'lambda_l2': np.linspace(1, 10, 10)\n",
    "            }\n",
    "\n",
    "param_static = {\n",
    "                'boosting_type': 'gbrt',\n",
    "                'objective': 'binary',\n",
    "                'metric': 'auc',\n",
    "                'max_bin': 1024,\n",
    "                'n_jobs': -1\n",
    "               }\n",
    "\n",
    "param_list = list(ParameterSampler(param_grid, \n",
    "                                   n_iter=10))\n",
    "\n",
    "rounded_list = [dict((k, round(v, 6)) for (k, v) in d.items())\n",
    "                for d in param_list]\n",
    "\n",
    "rounds = 10000\n",
    "early_stop_rounds = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, valid_index in skf.split(X, y):\n",
    "    X_train = X[train_index, :]\n",
    "    X_valid = X[valid_index, :]\n",
    "\n",
    "    y_train = df.loc[train_index, \"label\"]\n",
    "    y_valid = df.loc[valid_index, \"label\"]\n",
    "\n",
    "    d_train = lgb.Dataset(X_train, y_train, feature_name=cols)\n",
    "    d_valid = lgb.Dataset(X_valid, y_valid, feature_name=cols)    \n",
    "    \n",
    "    break\n",
    "\n",
    "results = []\n",
    "for i, params in enumerate(rounded_list):\n",
    "    print(params)\n",
    "    start_time = datetime.datetime.fromtimestamp(time.time())\n",
    "    \n",
    "    for key in param_static:\n",
    "        params[key] = param_static[key]\n",
    "    \n",
    "    model = lgb.train(params,\n",
    "                      d_train,\n",
    "                      num_boost_round=rounds,\n",
    "                      valid_sets=[d_train, d_valid],\n",
    "                      valid_names=['train','valid'],\n",
    "                      #feval=lgb_f1_score,\n",
    "                      early_stopping_rounds=early_stop_rounds,\n",
    "                      verbose_eval=0)\n",
    "\n",
    "    y_pred = model.predict(X_valid)\n",
    "    \n",
    "    end_time = datetime.datetime.fromtimestamp(time.time())\n",
    "    seconds = (end_time - start_time).total_seconds()\n",
    "    minutes = round(seconds/60, 1)\n",
    "    \n",
    "    res = auc(y_valid, y_pred)\n",
    "    print(i, \"ROC AUC:\", round(res, 4), \"F-score:\", \n",
    "          round(fbeta_score(y_valid, y_pred>0.5, beta=1), 4), \n",
    "          minutes, \"minutes\\n\")\n",
    "    \n",
    "    results.append((params, res))\n",
    "    for key in param_static:\n",
    "        del params[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "sorted_results[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
